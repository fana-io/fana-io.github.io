<!DOCTYPE html>
<html data-wf-page="5f71dd169010d6326b65485d">

<head>
  <meta charset="utf-8" />
  <title>Fána Feature Flags • Case Study</title>
  <meta content="width=device-width, initial-scale=1" name="viewport" />
  <link href="assets/css/style.css" rel="stylesheet" type="text/css" />
  <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js" type="text/javascript"></script>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lexend:regular,500,600,700" media="all" />
  <script type="text/javascript">
    WebFont.load({ google: { families: ["Lexend:regular,500,600,700"] } });
  </script>
  <script type="text/javascript">
    !(function (o, c) {
      var n = c.documentElement,
        t = " w-mod-";
      (n.className += t + "js"),
        ("ontouchstart" in o ||
          (o.DocumentTouch && c instanceof DocumentTouch)) &&
        (n.className += t + "touch");
    })(window, document);
  </script>
  <link href="assets/images/fana_graphic_color_forwhitebg.svg" rel="shortcut icon" type="image/x-icon" />
  <link href="assets/images/fana_graphic_color_forwhitebg.svg" rel="apple-touch-icon" />
  <script src="https://kit.fontawesome.com/d019875f94.js" crossorigin="anonymous"></script>
  <meta name="image" property="og:image" content="assets/images/fana_graphic_color_forwhitebg.svg" />
</head>

<body>
  <div class="navigation-wrap">
    <div data-collapse="medium" data-animation="default" data-duration="400" role="banner" class="navigation w-nav">
      <div class="navigation-container">
        <div class="navigation-left">
          <a href="/" aria-current="page" class="brand w-nav-brand w--current" aria-label="home">
            <img
            src="assets/images/fana_graphic_color_forwhitebg.svg"
            alt=""
            class="template-logo"
            width="50px"
          />
          </a>
          <nav role="navigation" class="nav-menu w-nav-menu">
            <a href="./case-study.html" class="link-block w-inline-block">
              <div>Case Study</div>
            </a>
            <a href="./team.html" class="link-block w-inline-block">
              <div>The Team</div>
            </a>
            <a href="/fana-docs/" class="link-block w-inline-block">
              <div>Docs</div>
            </a>
          </nav>
        </div>
        <div class="navigation-right">
          <div class="login-buttons">
            <a href="https://github.com/fana-io" target="_blank">
              <span style="color: #15416C">
                <i class="fab fa-github fa-lg"></i>
              </span>
            </a>
          </div>
        </div>
      </div>
      <div class="w-nav-overlay" data-wf-ignore="" id="w-nav-overlay-0"></div>
    </div>
  </div>
  <div class="cs-horizontal">
    <div id="sidebar" class="toc">
    </div>
    <div class="section header">
      <article class="container case-study-container">
        <div class="hero-text-container">
          <div id="case-study">
          <h1 class="h1 centered">Case Study</h1>
          <!-- Section 1 -->
          <h2 class="h2">1 Introduction</h2>
          <br>
          <p>
            Fána is an open-source feature flag management platform that facilitates testing in production. Software developers can release new features to targeted subsets of their user base, starting with their most bug-tolerant users like beta testers. By providing developers the means to toggle new features “on” for progressively larger audiences, Fána offers a tool with which to control testing in production safely and responsibly.
          </p>
          <br />
          <p>This case study will cover the tradeoffs of testing new features in production, how F&aacute;na enables it, and the engineering decisions the F&aacute;na team made during the implementation process.</p>  
          <br />
          <!-- <h3>1.1 Who’s interested in load test results?</h3>
          <br />
          <p>
            Load test results are valuable to both business stakeholders and engineering teams. 
          </p>
          </br>
          <p>
            On the business side, when an app can’t cope with its current amount of traffic, the app’s performance degrades or the app may stop working entirely. 
            This results in an immediate drop in the company’s revenue.
          </p>
          <br />
          <p>
            From the engineering team’s perspective, load testing provides valuable feedback about your production system - how many concurrent users can it handle? 
            How does response time change as load increases? Is there an uptick in failed requests under heavier load?
          </p>
          <br />
          <p>
            It’s this engineering perspective that we’ll focus on in this case study of Monsoon.
          </p>
          <br />
          <h3>1.2 What is Monsoon?</h3>
          <br />
          <p>
            Monsoon is an open-source, serverless framework for running browser-based load tests in the cloud. 
          </p>
          <br />
          <figure>
            <img src="assets/images/1-introduction/demo.gif" id="case-study" alt="">
            <figcaption>Fig 1.1: Monsoon's dashboard</figcaption>
          </figure>
          <br />
          <p>
            Monsoon allows software engineers to easily load test their single-page application in anticipation of traffic spikes or overall business growth. 
            Monsoon can simulate loads of up to 20,000 concurrent users, and tests can be of any duration, from minutes to hours or longer. 
            Engineers can also see their load test results visualized in a near real time dashboard.
          </p>
          <br />
          <h3>1.3 Brain Boost, a Hypothetical User Story</h3>
          <br />
          <p>
            To see load testing in action, let’s spend some time with the engineering team at Boost Health. 
            Boost is a rapidly-growing startup in the health and wellness space.
          </p>
          <br />
          <figure>
            <img src="assets/images/1-introduction/boost_team.svg" class="case-study-image" alt="">
            <figcaption>Fig 1.1: The Team</figcaption>
          </figure>
          <p>
            The stakes are high. Boost’s marketing team has spent months planning the product launch for its new Brain Boost supplement. 
            It’s Boost’s biggest product launch ever, and business executives want to close new rounds of venture capital funding based on the success of Brain Boost. 
          </p>
          <br />
          <figure>
            <img src="assets/images/1-introduction/brain.svg" class="case-study-image-x-small" alt="">
            <figcaption>Fig 1.2: Going all in on Brain Boost</figcaption>
          </figure>
          <br />
          <p>
            Boost expects more traffic than their site has ever seen on launch day. 
            They’re predicting peaks of 4,000 concurrent users. 
            The engineers at Boost are tasked with making sure the site can withstand all that traffic, so they decide to run load tests.
          </p>
          <br />
          <p>
            But before we discuss these load tests, it’s important to note that the Boost website is a <strong>single page application</strong>. 
            This has important implications for load testing.
          </p>
          <br />
          <p>A single page application is a web app comprised of just a single HTML page. Unlike a traditional website, after the initial page load, there are no page reloads. 
          Further dynamic updates to the page are handled via data-centric APIs, which rely on browser-side JavaScript for rendering. 
          </p>
          <br />
          
        
          <h3>1.4 Boost Health and Their Load Testing Journey</h3>
          <br />
          <p>
            So let’s follow along with the Boost engineers as they load test their SPA.
          </p>
          <br />
          <h4>First Attempt - Protocol-based load testing with JMeter</h4>

          <br />
          <p>
            One of the Boost engineers used a well-established, open-source load testing tool called Apache JMeter at a previous job, 
            so this is the first option the team tries.
          </p>
          <br />
          <figure>
            <img src="assets/images/1-introduction/protocol-based-load-testing.gif" class="case-study-image-large">
            <figcaption>Fig 1.4: Protocol-based load testing</figcaption>
          </figure>
          <p>
            JMeter is categorized as a <strong>protocol-based load testing</strong> tool. Protocol-based load tests are the original type of load test. 
            They involve traffic simulation at the HTTP protocol layer. 
            For example, if loading a webpage triggers HTTP requests for 75 subresources, with protocol-based testing, the developers will need to write code to request the original page AND all 75 of those subresources.
          </p>
          <br />
          <p>
            The Boost Health engineers want to test a customer adding Brain Boost to her cart, a process that breaks down into 3 different actions:
          </p>
          <ol>
            <li>Go to the Boost Health Main Page</li>
            <li>View the Brain Boost product details</li>
            <li>Add Brain Boost to the cart</li>
          </ol>
          <br/>
          <figure>
            <img src="assets/images/1-introduction/3actions_125requests.gif" class="case-study-image-large">
            <figcaption>Fig 1.5: Bye bye weekend</figcaption>
          </figure>
          <p>
            To simulate this workflow, the Boost engineers need to program JMeter to send 125 different HTTP requests in the correct order. This is clearly a great deal of work for the team. On top of this, JMeter is a complex tool and the learning curve is steep. 
          </p>
          <br />
          <p>
            After the team gets their first JMeter load test working, their results seem strangely incomplete. It turns out JMeter is a poor choice for load testing SPAs. 
            SPAs are JavaScript-intensive, but JMeter has no JavaScript interpreter and therefore can’t execute any JavaScript code. (The same holds true for other protocol-based load testing tools.) 
            Therefore, the bulk of an SPA is untestable with JMeter. This is a dealbreaker. 
          </p>
          <br />
          <h4>Second Attempt - Browser-based load testing with Selenium</h4>
          <br />
          <p>
            Another engineer on the Boost team knows of a different tool called Selenium and knows that Selenium can be used for browser-based load testing. 
          </p>
          <br />
          <p>
            <Strong>Browser-based load testing</strong> simulates web traffic using real web browsers rather than naked network requests. 
            Since we’re using browser instances to direct traffic to the site being tested, 
            those browsers clearly have built-in JavaScript interpreters and are fully capable of handling SPAs, unlike protocol-based load testing tools.
          </p>
          <br />
          <p>
            An even more fundamental difference between protocol-based load testing and browser-based load testing tools like Selenium is this: 
            Is the core unit under test the network request? Or is it an action the end user takes (which could actually result in 100 or more network requests)?
          </p>
          <br />
          <p>
            In Boost’s case, it’s an action a website user takes, like loading the homepage, 
            viewing product details or clicking the “Add to Cart” button.
          </p>
          <br />
          <p>
            Furthermore, thinking in terms of end user actions rather than lower-level network requests means the Boost engineers can think at a higher level of abstraction, which reduces bugs, makes for a better developer experience and saves significant amounts of developer time.
          </p>
          <br />
          <p>
            Returning to Selenium - it's a suite of browser-based test automation tools. It was never actually designed for load testing, but that's how Selenium came to be used by many developers. With Selenium, the Boost engineers don't need to worry about HTTP requests anymore. All they need to do is script their 3 end user actions ("Go to the Boost Health Main Page", "View the Brain Boost Product Details", "Add Brain Boost to the cart").​
          </p>
          <br />
          <figure>
            <img src="assets/images/1-introduction/3actions_3actions.gif" class="case-study-image-large">
            <figcaption>Fig 1.6: Ok, that's better</figcaption>
          </figure>
          <p> 
            The team runs their first browser-based load test using Selenium. 
            It’s a fairly small-scale test simulating 5 users concurrently visiting the Boost Health website. This test goes off without a hitch. 
          </p>
          <br />
          <p>
            Next, they test 100 concurrent users. This test doesn’t go so well. Selenium is a tool that runs locally on one of the engineers’ laptops, and testing 100 users means spinning up 100 browser instances. This is too resource-intensive for a single laptop. 
            And the actual number of users the Boost team needs to test is 4000, not 100. So the team has run out of local computing resources before they’re able to apply sufficient load to the Boost site.
          </p>
          <br />
          <figure>
            <img src="assets/images/1-introduction/unhappydev-selenium.svg" class="case-study-image">
            <figcaption>Fig 1.7: Local browser-based load testing doesn’t cut it</figcaption>
          </figure>
          <br />
          <p>
            Clearly this is unworkable. Because browser-based load testing is so resource intensive, the Boost engineers need a solution that’s hosted in the cloud.
          </p>
          <br />
          <h4>Third Attempt - Browser-based load testing with Flood</h4>
          <br />
          <p>
            Researching cloud-hosted, browser-based load testing, the team quickly discovers a platform called Flood. 
            Flood is an industry leader in the cloud-hosted, browser-based load testing space. Their platform is definitely capable of generating the 4000 concurrent users Boost needs to load test their site. However, Flood is very expensive. 
            We'll return to Flood later, but for now, the cost is a major drawback, enough to rule Flood out.
          </p>
          <figure>
            <img src="assets/images/1-introduction/unhappydev-flood.svg" class="case-study-image">
            <figcaption>Fig 1.8: Flood gets the job done but costs too much</figcaption>
          </figure>
          <br />
          <h4>The Journey So Far</h4>
          <p>
            Let's summarize where the Boost engineers are right now. They initially tried protocol-based load testing. This was unsuccessful because protocol-based tools can't test an SPA's JavaScript code. 
            Next, they tried local browser-based load testing. 
            This too was unsuccessful because browser-based load testing is too resource intensive for a single machine. Third, they tried browser-based load testing in the cloud with the Flood platform. 
            However, this proved prohibitively expensive. 
          </p>
          <br />
          <p>
            So the team searches for a more economical option for browser-based load testing in the cloud. 
            In short order, they come across an open source tool called Monsoon.
          </p>
          <br /> -->
          <!-- Section 2 -->
          <h2>2 Testing</h2>
          <br />
          <p>
            When introducing new features to end users, developers need a process that is optimized for the speed and reliability of the release. Modern development teams often integrate automated testing approaches into their deployment pipelines (such as Continuous Integration and Continuous Delivery pipelines) to create rapid, iterative workflows in which frequent code changes can be deployed faster. Automated testing suites can include tests that range from specific in scope, such as unit testing for single functions and components, to more broadly scoped tests that test interactions between the components of the system and prevent regression from newly merged code (e.g.integration tests and end-to-end-tests).
          </p>
          <p>
            Testing helps verify that an application as a whole behaves as intended and that the smaller components within the application communicate and integrate as expected<sup class="c15"><a href="#ftnt2" id="ftnt_ref2">[2]</a></sup>. In short, testing is the foundation required to "move fast with confidence"<sup class="c15"><a href="#ftnt3" id="ftnt_ref3">[3]</a></sup>
          </p>  
          <h3>2.1 Testing Before Production</h3>
          <p>
            While automated testing can prevent code regression, it does not always represent how real users may interact with a system or fully portray the environment in which the system will run. Staging environments attempt to replicate production with as high fidelity as possible to facilitate more robust and precise insight into the quality and behavior of the system at large. 
          </p>
          <img alt="" src="assets/images/image5.png" style="width: 288.75px; height: 245.718px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);"title="martin kelppmann quote">
          <br />
          <figure>
            <img alt="" src="assets/images/image44.png" title="staging environment">
            <figcaption>Fig. 2.1: Testing pipeline</figcaption>
          </figure>
          <p>
            While the staging environment aims to replicate the production environment for pre-production testing, many outside factors can influence the performance and quality of a system running in production that is difficult or cost-prohibitive to replicate. Examples include:
            <ul>
              <li >Race conditions</li>
              <li >Unpredictable user behavior</li>
              <li >Disparate runtime environments</li>
              <li >Unreliable networks, including external service dependencies</li>
              <li >Unpredictable user behavior</li>
           </ul>
          </p>
          <img alt="" src="assets/images/image23.png" style="width: 332.49px; height: 251.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="charity majors quote">
          <br />
          <p>
            Even with robust automated testing suites and sophisticated staging environments, the production environment still represents unpredictable and irreplicable conditions
          </p>
          <h3>2.2 Testing in Production Responsibly</h3>
          <br />
          <p>
            If the combination of external factors and user behavior in production environments are unique and difficult to replicate in controlled staging environments, developers will ultimately need to gain confidence in the reliability of their release in ways that pre-production environments cannot offer. A better way to ensure a system’s expected performance under real user strain and in real network conditions is to evaluate that system in a real production environment. Deliberately testing in production, then, is a powerful tool by which developers can boost an application’s resilience to survive errors and bugs.
          </p>
          <p>
            However, testing in production invariably poses some risk. Any change to a system can unexpectedly disrupt that system. Deployment failures can degrade the user experience, damage the reputation of both the product and the company, expose the company to regulatory scrutiny, or negatively impact the bottom line<sup class="c15"><a href="#ftnt4" id="ftnt_ref4">[4]</a></sup>
          </p>
          <p>
            Developers that test in production can leverage different tools to limit the negative outcomes in event that a new release doesn’t behave as intended. One pattern to help mitigate these risks is to roll out changes to users gradually: releasing new code to incrementally larger portions of the user base. This approach helps limit the blast radius, the reach that any problematic changes might cause, by increasingly exposing those changes only as confidence permits.
          </p>
          <br />
          <figure>
            <img alt="" src="assets/images/image42.png" title="gradual release">
            <figcaption>Fig. 2.2: Gradual release of features</figcaption>
          </figure>
          <br />
          <p>
            A further evolution of that approach is structuring the stages of that incremental release pattern to target specific types of users or audiences. For example, a new feature can be enabled solely for developers initially, then for a beta testing user group, then early adopters, etc., and finally to the general audience of all users. Exposing unproven portions of the application only to more bug-tolerant subsets of the user base first offers a safe way to mitigate the negative outcomes of a problematic release.
          </p>
          <br />
          <figure>
            <img alt="" src="assets/images/image1.png" title="targeting bug tolerant users">
            <figcaption>Fig. 2.3: Targeting Bug-tolerant Users</figcaption>
          </figure>
          <br />
          <p>
            As an example of this, when cloud computing company VMware started to outgrow an important conversation view feature in their app, engineers decided it was time for a total rewrite. While implementing the changes, VMware needed both to evaluate user feedback and investigate any issues experienced by real users, while also limiting its impact to the general audience of production users.
          </p>
          <br/>
          <figure>
            <img alt="" src="assets/images/image33.png" title="VMware case study">
            <figcaption>Fig. 2.4: VMware Case Study</figcaption>
          </figure>
          <p>
            To meet their requirements, VMware chose to release improvements of their feature rewrite to only a small group of targeted users at first. The release would be open to the general audience only after iterating with those selected users. The process was executed within the context of a continuous release cadence by way of selective testing in production<sup class="c23 c15"><a href="#ftnt5" id="ftnt_ref5">[5]</a></sup>.
          </p>
          <br/>
          <!-- Section 3-->
          <h2>3 Possible Solutions</h2>
          <br />
          <p>
            There are a few ways to target specific subsets of users with experimental features. For smaller teams looking to execute similar deployment approaches to that of VMware’s feature rewrite, two common approaches worth considering are Multiple Deployments and Feature Flags.
          </p>
          <h3>3.1 Multiple Deployments</h3>
          <br />
          <p>
            A multiple deployments strategy consists of cloning the production environment and implementing the new feature as a separate production version. A load balancer then takes care of routing the desired users between both the original and new environments. 
          </p>
          <figure>
            <img alt="" src="assets/images/image25.png" title="Multiple deployment">
            <figcaption>Fig. 3.1: Multiple Deployment</figcaption>
          </figure>
          </br>
          <p>
            This comes with the benefit of cleaner application code - since routing logic is abstracted to the load balancer, each environment is solely responsible for serving one version of the application. This ensures that the newer version of the application won’t contain code references to older versions, which reduces the potential for technical debt.
          </p>
          <p>
            Multiple deployment strategies like canary deployments also have the benefit of zero downtime for users: if something goes wrong with the new app, developers can route all traffic to the stable version. Also, since the routing logic can be configured in the load balancer, users can be selectively routed to specific experiences.
          </p>
          <p>
            There are some trade-offs to consider with multiple deployments. For example, if three new features are included in a deployment but one of the features is not performing as expected, the entire application, including the two satisfactory features, would need to be rolled back. 
          </p>
          <p>
            Additionally, deploying multiple production environments requires additional cloud or infrastructure resources since there is more than one production environment online. This adds cost and complexity to monitoring and maintaining the additional infrastructure. As the need for more versions of deployments grows, more environments would need to be added to test different variants of the application which would compound the cost, complexity, and overhead to manage the various deployments.
          </p>
          <p>
            The next section will explore using feature flags as an alternative approach to releasing a new feature.
          </p>
          <h3>3.2 Feature Flags</h3>
          <br />
          <p>
            A feature flag strategy, at the lowest level, consists of conditional statements that determine whether or not a given block of code should be executed. In more practical terms, a feature flag is a means by which to gate the execution of that code based on the context or environment in which it is evaluated. In the below example, if the `evaluateFlag` function for the `new_feature` evaluates to TRUE, the user receives the new feature; otherwise, they receive the old feature.
          </p>
          <figure>
            <img alt="" src="assets/images/image6.png" title="feature flag code">
            <figcaption>Fig 3.2 Feature Flag Code Snippet</figcaption>
          </figure>
          <p>
            Because feature flags rely on conditional flow in the source code itself, feature flags are usually configurable remotely, meaning that the codebase can be untouched while the routing or evaluation logic changes.
          </p>
          <p>
            As with multiple deployments, feature flags come with the benefits of zero downtime thanks to the ability to remotely toggle functionality. Additionally, since each new feature is tied to a single flag, this provides the advantage of selective rollbacks. If one out of three features is buggy, the developer can simply toggle that one feature off, while the others stay on.
          </p>
          <p>
            Since the conditional logic lives in the code, this adds maintenance complexity to the codebase. This feature flag lifecycle requires increased coordination between teams, including the status of active and inactive flags, when old flags can be removed, and who has ownership over the governance of different flags. If not carefully managed, feature flags can contribute to significant technical debt, so proper governance is a significant consideration.
          </p>
          <p>
            In addition to governance, there’s also the cost of additional loading time due to the network communication required to evaluate flags, since they’re managed remotely. 
          </p>
          <h3>3.3 Comparing Multiple Deployments and Feature Flags</h3>
          <br />
          <p>
            To revisit the key criteria for testing in production responsibly, the solution for a team looking to deploy a new feature with confidence needs to both facilitate strategic user group targeting and allow quick rollback of a problematic release with minimal downtime.
          </p>
          Further, to execute this approach as a smaller team looking to maintain high-velocity code deployment cycles, there are additional flexibility requirements:
          <ul>
            <li>Resolving an issue with one feature shouldn’t disrupt the deployment cycle of other changes. If possible, handling problematic features should have minimal collateral consequences for the deployment at large.</li>
            <li>Deployments and releases should be decoupled. By decoupling feature changes deployed to production from feature changes exposed to users, developer teams can reconcile long-term development with rapid iteration, as with VMware’s feature rewrite.</li>
          </ul>
          <p>
            While feature flags introduce additional complexity to a codebase, they present some distinct advantages as a deployment tool. While both a multiple deployments strategy and feature flagging strategy can accommodate targeted release audiences and expedient rollbacks, a unique benefit of a feature flag platform is the flexibility to isolate individual features to enable or disable them as needed.
          </p>
          <br />
          <!-- Section 4 -->
          <h2>4 Feature Flags for Testing in Production</h2>
          <br />
          <p>
            Feature flagging provides the ability to selectively enable and disable features in real-time, as well as the ability to designate specific audiences. This makes it a suitable tool for developers who wish to test their features in production. 
          </p>
          <p>
            There are various ways to use feature flags, including standard toggling, percentages, unique identifiers, and attribute/audience targeting.
          </p>
          <h3>4.1 Standard Toggling</h3>
          <br />
          <p>
            Feature flags offer the ability to remotely toggle functionality entirely on or off. This means that the developer can use a flag to release a new feature, and if something goes wrong, it can simply be toggled off.
          </p>
          <figure>
            <img alt="" src="assets/images/image13.png" title="standard toggle">
            <figcaption>Fig. 4.1: Standard Toggles</figcaption>
          </figure>
          <br />
          <p>
            However, this new feature would be released to the entire user base. Despite the speed of remotely toggling the feature off, wholesale release to the entire user base may still not be the best option because all users are potentially impacted by any problems. A standard toggle functionality isn’t quite enough - there needs to be a way to narrow down the number of impacted users.
          </p>
          <h3>4.2 Percentage Rollout</h3>
          <br />
          <p>
            One way to limit the impact is to adopt a percentage rollout strategy. This means users will be randomly sorted into the available experiences based on a predetermined rate. For example, the developer can have the new feature only serve 10% of their user base, limiting the potential impact of a negative experience.
          </p>
          <figure>
            <img alt="" src="assets/images/image24.png" title="percentage rollout">
            <figcaption>Fig 4.2 Percentage Rollout</figcaption>
          </figure>
          <br />
          <p>
            While this helps, percentage rollout does not provide granular control over which users will receive certain experiences. This may be more suited for a use case like A/B testing where the developer wants to be indiscriminate about who sees the experimental feature. A more responsible approach to testing in production would be to specifically target bug-tolerant users.
          </p>
          <h3>4.3 Unique Identifier</h3>
          <br />
          <p>
            One way to target bug-tolerant users is to designate a flag rule to target users based on a list of unique identifiers. This identifier can be anything that the developer has access to, like an user ID or IP address.
          </p>
          <br />
          <figure>
            <img alt="" src="assets/images/image29.png" title="conditions screenshot">
            <figcaption>Fig 4.3 Sample Conditions</figcaption>
          </figure>
          <br />
          <p>
            This may allow developers to target specific, bug-tolerant users at an extremely granular level, but it can become cumbersome as the target user list increases. This can bloat the flag ruleset and make it difficult to manage when targets need to be removed.
          </p>
          <h3>4.4 Attributes</h3>
          <br />
          <p>
            Feature flags can also be evaluated based on developer-defined attributes. These attributes can be general user information, session context, environment values, etc, used to conditionally execute application logic. Attributes facilitate a more flexible and granular approach by providing a means for deliberate criteria-based targeting according to whatever evaluation context values a developer chooses.
          </p>
          <figure>
            <img alt="" src="/assets/images/image45.png" title="">
            <figcaption>Fig 4.4: Target features to specific users by specifying attributes</figcaption>
          </figure>
          <p>As a simple example, a developer may be trying to target west coast students for an experimental new feature. Their flag would target anyone that has their <code>student</code> attribute as <code>true</code> and their <code>state</code> attribute as either <code>California</code>, <code>Washington</code>, or <code>Oregon</code>.</p>
          <p>To take it a step further, developers can bundle multiple attribute-based rules into groups, also known as audiences. Audiences are a set of rules based on those developer-defined attributes, and they offer convenient reuse of targeting criteria composed of more complexly structured or numerous attribute qualifications. This can be particularly useful when testing in production, as a single audience can be used for any flags without needing to redeclare any of the conditional attribute logic.</p>
          <figure>
            <img alt="" src="assets/images/image15.png">
            <figcaption>Fig 4.5: Target features to specific users by specifying attributes</figcaption>
          </figure>
          
          
          <p>If the example feature flag is modified to target West Coast Students, a developer can bundle the <code>student</code> and <code>state</code> attribute together to create this audience. Now, instead of having to set <code>student</code> to <code>true</code>, and <code>state</code> is <code>California</code>, <code>state</code> is <code>Washington</code>, or <code>state</code> is <code>Oregon</code> on every flag, developers can simply declare those requirements on the <code>West Coast Students</code> audience, and reuse the audience in the relevant flags. </p>
          <p>To revisit the case of VMware as a practical example, the engineering team utilized feature flags wherever the old feature was referenced in the existing code base, allowing different versions to be served to different users.</p>
          <p>To implement this, VMware created three separate user audiences: Developers, Beta users, and General Audience. Different versions of the feature were served by evaluating the respective feature flags based on which audience the user was in. This allowed the engineering team to test new iterations of the feature in production as they were being developed.</p>
          <figure>
            <img alt="" src="/assets/images/image16.png"title="">
            <figcaption>Fig 4.6: VMWare progressively rolled out a new feature to targeted user groups before releasing to all users</figcaption>
          </figure>
          <p>Once a feature completed the requisite internal testing, it was made available to Beta target users. This allowed VMware to collect user feedback and update the feature accordingly. Because of the size and scope of the feature, VMware iterated through several testing cycles within the Beta users before moving forward to the General Audience stage in production. Still, even once the feature was generally available, it could be easily disabled via toggling off the associated feature flag. This would effectively revert to the pre-rewrite version of the feature while issues with the new version could be investigated and resolved.</p>
          <br />
          <!-- Section 5 -->
          <h2>5 Implementation Options</h2>
          <br />
          <p>
            There are several options for teams looking to release features to targeted audiences using a feature flagging solution:  DIY in-house implementations, commercial options, and open source options. Each of these options has distinct strengths and limitations. 
          </p>
          <br />
          <h3>5.1 DIY</h3>
          <br />
          <p>
            One option to implement a feature flag platform is to build an in-house solution. To build one that accommodates audience-targeted releases, a DIY solution would need the following core features:
          </p>
          <ul>
            <li>flags need to be evaluated dynamically and in real-time to execute rapid rollback of problematic features</li>
            <li>a consistent data source to hold flag definitions and audience rules; i.e., which users see which features</li>
          </ul>
          <p>
            Architecturally, the above requirements necessitate several components to implement a minimum viable feature flagging platform:
          </p>
          <ul>
            <li>a persistent backend data store for flag data, audience rules, and the targeted attributes that compose those rules.</li>
            <li>a developer interface, like on a command line, by which to manage that data set: toggle flags, adjust the composition of audiences, etc.
            </li>
            <li>a package embedded in the application to execute the appropriate flagged features within the application.</li>
          </ul>
          <figure>
            <img alt="" src="/assets/images/image19.png"title="" style="max-width: 500px;">
            <figcaption>Fig 5.1: Minimum necessary components to build an in-house feature flag solution</figcaption>
          </figure>
          <p>Going the DIY route is a viable option, however, building a bootstrapped feature flagging system poses many challenges. For example, Atlassian built its own system primarily to control feature releases. As more teams wanted to use the in-house solution, developing and maintaining the solution became unwieldy. In addition, the system was not designed for users outside the development team. Without a user interface, teams outside of engineering, such as Product Management, had to rely on the engineering teams to manage flags and run beta tests<sup><a href="#ftnt6">[6]</a></sup>.</p>
          <p>While building an in-house feature flagging platform provides deployment flexibility and data privacy, there’s a significant cost in the engineering team’s available resources to build the tool out initially. It’s also likely that continuous work may be required to maintain and evolve the platform moving forward. </p>
          <p>While building an in-house feature flagging platform provides deployment flexibility and data privacy, there’s a significant cost in the engineering team’s available resources to build the tool out initially. It’s also likely that continuous work may be required to maintain and evolve the platform moving forward. </p>
          <h3>5.2 Open Source</h3>
          <br />
          <p>
            Leveraging open-source solutions is a cost-effective way to build a customizable feature flagging platform while maintaining data ownership through self-hosting. However, there are a few notable tradeoffs of using open-source solutions:
          </p>
          <ul>
            <li>Fewer features out of the box than paid solutions (e.g. experimentation, A/B testing, workflow integrations)</li>
            <li>Dedicated resources are needed to build and maintain the feature flag platform</li>
          </ul>
          <p>
            For example, FeatureHub offers the ability to implement targeting rules based on attributes but can't group those rules into audiences. The lack of reusability results in a tedious process of manually setting up rules on each flag. Unleash does have audience targeting (they call them segments), but only as part of their paid tier plan. Therefore, building off these open source solutions still requires additional resources to implement flexible and reusable audience targeting functionality.
          </p>
          <h3>5.3 Paid Solutions</h3>
          <br />
          <p>
            If the resource overhead or lead time of implementing a DIY solution is too prohibitive, there are several robust feature flag solutions on the market offering paid options. These save time and resources by providing rich and convenient functionality out of the box. There are a few options here, but two of the most popular ones on the market are LaunchDarkly and Optimizely.</p><p>Premium solutions come with a lot of built-in support for experimentation, workflow integrations, and segment targeting. However, with these benefits come some considerations:
          </p>
          <ul>
            <li>Paid: These enterprise solutions generally come at a monthly cost per user, which may be prohibitive to smaller companies</li>
            <li>Not self-hosted: Flag data and targeting rules may contain sensitive information, which can be problematic in cases where there are legal restrictions on sharing data with a third-party service</li>
          </ul>
          <h3>5.4 Weighing the Options</h3>
          <br />
          <p>
            A developer team looking for a high degree of flexibility and customization can opt for open source solutions to bootstrap a DIY feature flag implementation or build an in-house solution completely from the ground up. While flexible and customizable, the DIY route may carry a higher cost in the way of engineering resources and time.</p><p>For broad and proven functionality out of the box, a developer team can consider enterprise solutions like LaunchDarkly. However, some tradeoffs with using third-party services include:</p>
<ul>
  <li>Cost: certain features like targeting specific audiences are accessible through a monthly fee.</li>
  <li>Data privacy: Potentially exposing user data or company data to a third-party database.</li>
</ul><figure>
  <img alt="" src="assets/images/image28.png" title="">
  <figcaption>Fig 5.4: Comparison between DIY and paid feature flagging solutions</figcaption>
</figure>
<p>For a smaller, budget-conscious developer team that needs audience targeting functionality for testing in production out of the box and prioritizes data privacy, an enterprise solution may not meet their needs.</p>
<p>The next section introduces a Fána, an alternative feature flagging solution.</p>
  
          <!-- Section 6 -->
          <h2>6 Fana</h2>
          <br />
          <p>
            Fána occupies a niche in between, offering the ready-made functionality of a third-party solution for development teams specifically looking to utilize feature flagging to target audiences. 
          </p>
          <p>Fána offers a simple, straightforward frontend interface to manage flags and their audiences, making for an intuitive user experience with a minimal learning curve. Additionally, while Fána can be self-hosted to maximize data hosting flexibility, the option of automated deployment to Amazon Web Services (AWS) offers the most convenient solution to get up and running most quickly.</p>
          <figure>
            <img alt="" src="assets/images/image35.png">
            <figcaption>Fig 6.0: Comparing Fána as an alternative solution</figcaption>
          </figure>
          <br />
          <p>Fána allows developers to selectively serve experiences to particular audiences while allowing for remote toggling in real-time. This is particularly useful when testing in production, as developers may wish to only serve a new feature to bug-tolerant groups. If something goes wrong, developers can flip the switch to turn that feature off instantly. </p>
          <p>To implement audience targeting via feature flags, Fána makes use of a set of entities to describe, assess, and target users as discussed in the following sections.</p>
          <h3>6.1 Fana's Entities</h3>
          <h3>6.2 Architecture Overview</h3>
          <h3>6.3 Manager Platform</h3>
          <h3>6.4 Flag Bearer</h3>
          <h3>6.5 Redis Store and Publisher/Subscriber</h3>
          <h3>6.6 Software Development Kits</h3>
          <h3>6.7 Final Architecture/h3>
            
          <h2>7 Deploying Fana</h2>
          <br />
          <h2>Docker</h2>
          <h2>AWS</h2>
          <br />

        <!-- Section 7 -->
        <h2>8 Engineering Decisions & Tradeoffs</h2>
        <br />
        <p>
    
        </p>
        <h3>SQL vs NoSQL</h3>
        <h3>Decoupling Manager Responsibilities</h3>
        <h3>Communicating Between Components</h3>
        <br />

        <!-- Section 8 -->
        <h2>9 Summary & Future Work</h2>
        <br />
        <p>
    
        </p>
        <h2>Edge Computing</h2>
        <h2>Supporting Multiple Environments</h2>
        <h2>Collecting Metrics</h2>
        <br />
  
          <!-- References -->
          <h2>References</h2>
          <div>
            <p><a href="#ftnt_ref1" id="ftnt1">[1]</a>Web Scalability for Startup Engineers p333</p>
         </div>
         <div>
            <p><a href="#ftnt_ref2" id="ftnt2">[2]</a>https://martinfowler.com/articles/microservice-testing/</p>
         </div>
         <div>
            <p><a href="#ftnt_ref3" id="ftnt3">[3]</a>https://blog.samstokes.co.uk/blog/2016/07/11/move-fast-with-confidence/</p>
         </div>
         <div>
            <p><a href="#ftnt_ref4" id="ftnt4">[4]</a>https://increment.com/testing/what-broke-the-bank/</p>
         </div>
         <div>
            <p><a href="#ftnt_ref5" id="ftnt5">[5]</a>https://medium.com/vmware-end-user-computing/using-feature-flags-to-enable-nearly-continuous-deployment-for-mobile-apps-b6d0940657ff</p>
         </div>
         <div>
            <p><a href="#ftnt_ref6" id="ftnt6">[6]</a>https://launchdarkly.com/case-studies/atlassian/</p>
         </div>
         <div>
            <p><a href="#ftnt_ref7" id="ftnt7">[7]</a>https://docs.featurehub.io/featurehub/latest/architecture.html#_nats</p>
         </div>
         <div>
            <p><a href="#ftnt_ref8" id="ftnt8">[8]</a>https://launchdarkly.com/blog/flag-delivery-at-edge/</p>
         </div>
          <!-- <p><strong>Books</strong></p>
          <ul>
            <li><a href="https://www.amazon.com/Hitchhiking-Guide-Testing-Projects-Step/dp/0988540207" class="references">The Hitchhiking Guide To Load Testing Projects by Leandro Melendez</a></li>
            <li><a href="https://www.oreilly.com/library/view/data-pipelines-pocket/9781492087823/"
                class="references">O’Reilly’s Data Pipelines Pocket Reference by James Densmore</a></li>
            <li><a href="https://www.oreilly.com/library/view/designing-data-intensive-applications/9781491903063/" class="references">O’Reilly’s Designing Data-Intensive Applications by Martin Kleppmann</a></li>
          </ul>
          <p><strong>Articles</strong></p>
          <ul>
            <li>
              <a
                href="https://addyosmani.com/blog/puppeteer-recipes/"
                class="references">Web Performance Recipes With Puppeteer by Addy Osmani
              </a>
            </li>
            <li>
              <a
                href="https://aws.amazon.com/solutions/implementations/distributed-load-testing-on-aws/"
                class="references">AWS Distributed Load Testing Guide
              </a>
            </li>
            <li>
              <a
                href="https://machinelearningmastery.com/moving-average-smoothing-for-time-series-forecasting-python/"
                class="references">Moving Average Smoothing for Data Preparation and Time Series Forecasting in Python by Jason Brownlee, PhD
              </a>
            </li>
          </ul>
          <p><strong>Podcasts</strong></p>
          <ul>
            <li><a href="https://softwareengineeringdaily.com/2018/02/08/load-testing-mobile-applications-with-paulo-costa-and-rodrigo-coutinho/" class="references">Software Engineering Daily, February 8, 2018</a></li>
            <li><a href="https://softwareengineeringdaily.com/2017/03/08/load-testing-with-mark-gilbert/" class="references">Software Engineering Daily, March 8, 2017</a></li>
          </ul>
          <p><strong>Companies:</strong></p>
          <ul>
            <li><a href="https://www.flood.io" class="references">Flood</a></li>
            <li><a href="https://www.loadview-testing.com" class="references">loadview</a></li>
            <li><a href="https://loadninja.com" class="references">LoadNinja</a></li>
          </ul> -->
          <br>
          <h2>Presentation</h2>
          <div class="presentation-wrapper case-study-image">

            <iframe class="presentation-video" frameborder="0" src=""
              allowfullscreen></iframe>
          </div class="presentation-wrapper">
          <br>
          <h2>Team</h2>
          <br />
          <div class="section team-section">
            <div class="container">
              <div data-duration-in="300" data-duration-out="100" class="tabs w-tabs">
                <div data-w-id="8ce4324a-ed8e-4436-9964-0cfbaf67c64a"
                  style="transform: translate3d(0px, 55px, 0px) scale3d(1, 1, 1) rotateX(0deg) rotateY(0deg) rotateZ(0deg) skew(0deg, 0deg); transform-style: preserve-3d; opacity: 0;"
                  class="tabs-content w-tab-content">
                  <div>
                    <div class="team-grid">
                      <div class="team-member-wrap">
                        <img
                          src="assets/images/team/audry.png"
                          loading="lazy"
                          alt=""
                        />
                        <div class="team-member-info">
                          <div class="team-member-name">Audry Hsu</div>
                          <div class="team-member-location">Seattle, WA</div>
                        </div>
                        <ul class="team-member-icons">
                          <li>
                            <a href="mailto:audry.hsu@gmail.com" target="_blank">
                              <span class="team-member-icon">
                                <i class="fas fa-envelope"></i>
                              </span>
                            </a>
                          </li>
                          <li>
                            <a href="https://www.audryhsu.com" target="_blank">
                              <span class="team-member-icon">
                                <i class="fas fa-globe"></i>
                              </span>
                            </a>
                          </li>
                          <li>
                            <a
                              href="https://www.linkedin.com/in/audry-hsu"
                              target="_blank"
                            >
                              <span class="team-member-icon">
                                <i class="fab fa-linkedin"></i>
                              </span>
                            </a>
                          </li>
                        </ul>
                      </div>
                      <div class="team-member-wrap">
                        <img
                          src="assets/images/team/juan.jpg"
                          loading="lazy"
                          alt=""
                        />
                        <div class="team-member-info">
                          <div class="team-member-name">Juan Juy</div>
                          <div class="team-member-location">Los Angeles, CA</div>
                        </div>
                        <ul class="team-member-icons">
                          <li>
                            <a href="mailto:alexjuanjuy@gmail.com" target="_blank">
                              <span class="team-member-icon">
                                <i class="fas fa-envelope"></i>
                              </span>
                            </a>
                          </li>
                          <li>
                            <a href="https://www.juanjuy.com/" target="_blank">
                              <span class="team-member-icon">
                                <i class="fas fa-globe"></i>
                              </span>
                            </a>
                          </li>
                          <li>
                            <a
                              href="https://www.linkedin.com/in/juanjuy/"
                              target="_blank"
                            >
                              <span class="team-member-icon">
                                <i class="fab fa-linkedin"></i>
                              </span>
                            </a>
                          </li>
                        </ul>
                      </div>
                      <div class="team-member-wrap">
                        <img
                          src="assets/images/team/rob.jpg"
                          loading="lazy"
                          alt=""
                        />
                        <div class="team-member-info">
                          <div class="team-member-name">Rob Gorman</div>
                          <div class="team-member-location">Boston, MA</div>
                        </div>
                        <ul class="team-member-icons">
                          <li>
                            <a
                              href="mailto:gorman.rob.j@gmail.com"
                              target="_blank"
                            >
                              <span class="team-member-icon">
                                <i class="fas fa-envelope"></i>
                              </span>
                            </a>
                          </li>
                          <li>
                            <a href="https://robgorman.com" target="_blank">
                              <span class="team-member-icon">
                                <i class="fas fa-globe"></i>
                              </span>
                            </a>
                          </li>
                          <li>
                            <a
                              href="https://www.linkedin.com/in/robjgorman/"
                              target="_blank"
                            >
                              <span class="team-member-icon">
                                <i class="fab fa-linkedin"></i>
                              </span>
                            </a>
                          </li>
                        </ul>
                      </div>
                      <div class="team-member-wrap">
                        <img
                          src="assets/images/team/yoorhim.jpg"
                          loading="lazy"
                          alt=""
                        />
                        <div class="team-member-info">
                          <div class="team-member-name">Yoorhim Choi</div>
                          <div class="team-member-location">New York, NY</div>
                        </div>
                        <ul class="team-member-icons">
                          <li>
                            <a href="mailto:choi.yoorhim@gmail.com" target="_blank">
                              <span class="team-member-icon">
                                <i class="fas fa-envelope"></i>
                              </span>
                            </a>
                          </li>
                          <li>
                            <a href="https://www.yoorhim.com" target="_blank">
                              <span class="team-member-icon">
                                <i class="fas fa-globe"></i>
                              </span>
                            </a>
                          </li>
                          <li>
                            <a
                              href="https://www.linkedin.com/in/yoorhimchoi/"
                              target="_blank"
                            >
                              <span class="team-member-icon">
                                <i class="fab fa-linkedin"></i>
                              </span>
                            </a>
                          </li>
                        </ul>
                      </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
          <br>
          <br>
    </div>
  </article>
  </div>
  <script src="https://d3e54v103j8qbb.cloudfront.net/js/jquery-3.5.1.min.dc5e7f18c8.js?site=5f71dd169010d641cf65485c"
    type="text/javascript" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0="
    crossorigin="anonymous"></script>
  <script src="https://assets.website-files.com/5f71dd169010d641cf65485c/js/webflow.6af2032ff.js"
    type="text/javascript"></script>
  <script>
    /*!
     * toc - jQuery Table of Contents Plugin
     * v0.3.2
     * http://projects.jga.me/toc/
     * copyright Greg Allen 2014
     * MIT License
    */
    !function (a) { a.fn.smoothScroller = function (b) { b = a.extend({}, a.fn.smoothScroller.defaults, b); var c = a(this); return a(b.scrollEl).animate({ scrollTop: c.offset().top - a(b.scrollEl).offset().top - b.offset }, b.speed, b.ease, function () { var a = c.attr("id"); a.length && (history.pushState ? history.pushState(null, null, "#" + a) : document.location.hash = a), c.trigger("smoothScrollerComplete") }), this }, a.fn.smoothScroller.defaults = { speed: 400, ease: "swing", scrollEl: "body,html", offset: 0 }, a("body").on("click", "[data-smoothscroller]", function (b) { b.preventDefault(); var c = a(this).attr("href"); 0 === c.indexOf("#") && a(c).smoothScroller() }) }(jQuery), function (a) { var b = {}; a.fn.toc = function (b) { var c, d = this, e = a.extend({}, jQuery.fn.toc.defaults, b), f = a(e.container), g = a(e.selectors, f), h = [], i = e.activeClass, j = function (b, c) { if (e.smoothScrolling && "function" == typeof e.smoothScrolling) { b.preventDefault(); var f = a(b.target).attr("href"); e.smoothScrolling(f, e, c) } a("li", d).removeClass(i), a(b.target).parent().addClass(i) }, k = function () { c && clearTimeout(c), c = setTimeout(function () { for (var b, c = a(window).scrollTop(), f = Number.MAX_VALUE, g = 0, j = 0, k = h.length; k > j; j++) { var l = Math.abs(h[j] - c); f > l && (g = j, f = l) } a("li", d).removeClass(i), b = a("li:eq(" + g + ")", d).addClass(i), e.onHighlight(b) }, 50) }; return e.highlightOnScroll && (a(window).bind("scroll", k), k()), this.each(function () { var b = a(this), c = a(e.listType); g.each(function (d, f) { var g = a(f); h.push(g.offset().top - e.highlightOffset); var i = e.anchorName(d, f, e.prefix); if (f.id !== i) { a("<span/>").attr("id", i).insertBefore(g) } var l = a("<a/>").text(e.headerText(d, f, g)).attr("href", "#" + i).bind("click", function (c) { a(window).unbind("scroll", k), j(c, function () { a(window).bind("scroll", k) }), b.trigger("selected", a(this).attr("href")) }), m = a("<li/>").addClass(e.itemClass(d, f, g, e.prefix)).append(l); c.append(m) }), b.html(c) }) }, jQuery.fn.toc.defaults = { container: "body", listType: "<ul/>", selectors: "h1,h2,h3", smoothScrolling: function (b, c, d) { a(b).smoothScroller({ offset: c.scrollToOffset }).on("smoothScrollerComplete", function () { d() }) }, scrollToOffset: 0, prefix: "toc", activeClass: "toc-active", onHighlight: function () { }, highlightOnScroll: !0, highlightOffset: 100, anchorName: function (c, d, e) { if (d.id.length) return d.id; var f = a(d).text().replace(/[^a-z0-9]/gi, " ").replace(/\s+/g, "-").toLowerCase(); if (b[f]) { for (var g = 2; b[f + g];)g++; f = f + "-" + g } return b[f] = !0, e + "-" + f }, headerText: function (a, b, c) { return c.text() }, itemClass: function (a, b, c, d) { return d + "-" + c[0].tagName.toLowerCase() } } }(jQuery);
  </script>
  <script>
    /* initialize */
    $('.toc').toc({
      'selectors': 'h2', //elements to use as headings
      'container': 'article', //element to find all selectors in
      'smoothScrolling': true, //enable or disable smooth scrolling on click
      'prefix': 'toc', //prefix for anchor tags and class names
      'highlightOnScroll': true, //add class to heading that is currently in focus
      'highlightOffset': 100, //offset to trigger the next headline
    });
  </script>
</body>

</html>